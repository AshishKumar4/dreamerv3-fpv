defaults:

  logdir: ~/logdir/{timestamp}
  replica: 0
  replicas: 1
  method: name
  task: dummy_disc
  seed: 0
  script: train
  batch_size: 16
  batch_length: 64
  report_length: 32
  consec_train: 1
  consec_report: 1
  replay_context: 1
  random_agent: False
  demo_data_dir: ''
  clock_addr: ''
  clock_port: ''
  ipv6: False
  errfile: False

  logger:
    outputs: [jsonl, scope]
    filter: 'score|length|fps|ratio|train/loss/|train/rand/'
    timer: True
    fps: 15
    user: ''

  env:
    atari: {size: [96, 96], repeat: 4, sticky: True, gray: True, actions: all, lives: unused, noops: 30, autostart: False, pooling: 2, aggregate: max, resize: pillow, clip_reward: False}
    procgen: {size: [96, 96], resize: pillow}
    crafter: {size: [64, 64], logs: False}
    atari100k: {size: [64, 64], repeat: 4, sticky: False, gray: False, actions: needed, lives: unused, noops: 30, autostart: False, resize: pillow, clip_reward: False}
    dmlab: {size: [64, 64], repeat: 4, episodic: True, use_seed: True}
    minecraft: {size: [64, 64], break_speed: 100.0, logs: False, length: 36000}
    dmc: {size: [64, 64], repeat: 1, proprio: True, image: True, camera: -1}
    loconav: {size: [64, 64], repeat: 1, camera: -1}
    fpv:
      simulator: colosseum
      reward_type: navigation
      waypoint_type: visual
      size: [96, 96]
      control_hz: 30
      max_steps: 2000
      use_depth: false
      # Control
      roll_scale: 2.0
      pitch_scale: 2.0
      yaw_scale: 2.0
      throttle_hover: 0.35
      effective_range: 0.5
      # Reward
      gate_radius: 1.6
      gate_bonus: 60.0
      gate_bonus_decay: true
      gate_size: 1.6
      gate_bonus_progressive: true
      gate_bonus_max_scale: 3.0
      collision_penalty: -25.0
      progress_scale: 5.0
      progress_clip_min: -1
      progress_clip_max: 1
      survival_bonus: 0.1
      survival_decay: 200.0
      time_penalty: -0.002
      time_penalty_growth: 300.0
      rate_penalty_scale: 0.3
      rate_penalty_control_hz: 30.0
      rate_yaw_weight: 1
      jerk_penalty_scale: 0.001
      visibility_penalty: 0.5
      camera_fov: 100.0
      # Waypoints
      visible_waypoints: 5
      show_spline: true
      loop_start_index: 0
      static_gates:
        - "8.0,0.0,-3.0"
        - "18.0,0.0,-5.0"
        - "30.0,0.0,-5.0"
        - "42.0,3.0,-5.0"
        - "52.0,8.0,-5.0"
        - "58.0,15.0,-5.0"
        - "58.0,30.0,-5.0"
        - "58.0,44.0,-5.0"
        - "52.0,54.0,-5.0"
        - "42.0,60.0,-5.0"
        - "28.0,60.0,-5.0"
        - "14.0,60.0,-5.0"
      # Randomization
      waypoint_randomization:
        enabled: true
        xy_radius: 5.0
        z_range: [-2.0, 2.0]
        min_altitude: -3.0
        max_altitude: -10.0
        min_waypoint_distance: 8.0
        smoothing_factor: 0.6
        seed: null
      # Backends
      colosseum:
        vehicle_name: ""
        markers:
          target_asset: "BP_RedMarker"
          waypoint_asset: "BP_GrayMarker"
          spline_asset: "Sphere"
          target_scale: 0.6
          waypoint_scale: 0.35
          spline_scale: 0.12
          target_is_blueprint: true
          waypoint_is_blueprint: true
          spline_is_blueprint: false

  replay:
    size: 5e6
    online: True
    fracs: {uniform: 1.0, priority: 0.0, recency: 0.0}
    prio: {exponent: 0.8, maxfrac: 0.5, initial: inf, zero_on_sample: True}
    priosignal: model
    recexp: 1.0
    chunksize: 1024

  run:
    steps: 1e10
    duration: 0
    train_ratio: 128.0
    log_every: 120
    report_every: 300
    save_every: 900
    video_every: 4096
    envs: 1
    eval_envs: 1
    eval_eps: 1
    report_batches: 1
    from_checkpoint: ''
    from_checkpoint_regex: '^(?!aux_state|aux_goal|dec/vec|dec/mlp|enc/mlp)'
    episode_timeout: 180
    actor_addr: 'localhost:{auto}'
    replay_addr: 'localhost:{auto}'
    logger_addr: 'localhost:{auto}'
    actor_batch: -1
    actor_threads: 1
    agent_process: False
    remote_replay: False
    remote_envs: False
    usage: {psutil: True, nvsmi: True, gputil: False, malloc: False, gc: False}
    debug: False

  jax:
    platform: cuda
    compute_dtype: bfloat16
    policy_devices: [0]
    train_devices: [0]
    policy_on_cpu: False
    mock_devices: 0
    prealloc: True
    precompile: True
    jit: True
    debug: False
    expect_devices: 0
    enable_policy: True
    coordinator_address: ''

  agent:
    loss_scales: {rec: 1.0, rew: 1.0, con: 1.0, dyn: 1.0, rep: 0.1, policy: 1.0, value: 1.0, repval: 0.3, aux_state: 0.0, aux_goal: 0.0}
    opt: {lr: 4e-5, agc: 0.3, eps: 1e-20, beta1: 0.9, beta2: 0.999, momentum: True, wd: 0.0, schedule: const, warmup: 1000, anneal: 0}
    ac_grads: False
    aux_state_head: {layers: 2, units: 512, act: silu, norm: rms, output: mse, outscale: 1.0, winit: trunc_normal_in}
    aux_goal_head: {layers: 2, units: 256, act: silu, norm: rms, output: mse, outscale: 1.0, winit: trunc_normal_in}
    dyn:
      typ: rssm
      rssm: {deter: 8192, hidden: 1024, stoch: 32, classes: 64, act: silu, norm: rms, unimix: 0.01, outscale: 1.0, winit: trunc_normal_in, imglayers: 2, obslayers: 1, dynlayers: 1, absolute: False, blocks: 8, free_nats: 1.0}
    enc:
      typ: simple
      simple: {depth: 64, mults: [2, 3, 4, 4], layers: 3, units: 1024, act: silu, norm: rms, winit: trunc_normal_in, symlog: True, outer: False, kernel: 5, strided: False}
    dec:
      typ: simple
      simple: {depth: 64, mults: [2, 3, 4, 4], layers: 3, units: 1024, act: silu, norm: rms, outscale: 1.0, winit: trunc_normal_in, outer: False, kernel: 5, bspace: 8, strided: False}
    rewhead: {layers: 1, units: 1024, act: silu, norm: rms, output: symexp_twohot, outscale: 0.0, winit: trunc_normal_in, bins: 255}
    conhead: {layers: 1, units: 1024, act: silu, norm: rms, output: binary, outscale: 1.0, winit: trunc_normal_in}
    policy: {layers: 3, units: 1024, act: silu, norm: rms, minstd: 0.1, maxstd: 1.0, outscale: 0.01, unimix: 0.01, winit: trunc_normal_in}
    value: {layers: 3, units: 1024, act: silu, norm: rms, output: symexp_twohot, outscale: 0.0, winit: trunc_normal_in, bins: 255}
    policy_dist_disc: categorical
    policy_dist_cont: bounded_normal
    imag_last: 0
    imag_length: 20
    horizon: 900
    contdisc: True
    imag_loss: {slowtar: False, lam: 0.95, actent: 1e-3, slowreg: 1.0}
    repl_loss: {slowtar: False, lam: 0.95, slowreg: 1.0}
    slowvalue: {rate: 0.02, every: 1}
    retnorm: {impl: perc, rate: 0.01, limit: 1.0, perclo: 5.0, perchi: 95.0, debias: False}
    valnorm: {impl: none, rate: 0.01, limit: 1e-8}
    advnorm: {impl: none, rate: 0.01, limit: 1e-8}
    reward_grad: True
    repval_loss: True
    repval_grad: True
    report: True
    report_gradnorms: False

size1m: &size1m
  .*\.rssm: {deter: 512, hidden: 64, classes: 4}
  .*\.depth: 4
  .*\.units: 64

size12m: &size12m
  .*\.rssm: {deter: 2048, hidden: 256, classes: 16}
  .*\.depth: 16
  .*\.units: 256

size25m: &size25m
  .*\.rssm: {deter: 3072, hidden: 384, classes: 24}
  .*\.depth: 24
  .*\.units: 384

size50m: &size50m
  .*\.rssm: {deter: 4096, hidden: 512, classes: 32}
  .*\.depth: 32
  .*\.units: 512

size100m: &size100m
  .*\.rssm: {deter: 6144, hidden: 768, classes: 48}
  .*\.depth: 48
  .*\.units: 768

size200m: &size200m
  .*\.rssm: {deter: 8192, hidden: 1024, classes: 64}
  .*\.depth: 64
  .*\.units: 1024

size400m: &size400m
  .*\.rssm: {deter: 12288, hidden: 1536, classes: 96}
  .*\.depth: 96
  .*\.units: 1536

minecraft:
  task: minecraft_diamond

dmlab:
  task: dmlab_explore_goal_locations_small
  run: {steps: 2.6e7, train_ratio: 32}

atari:
  task: atari_pong
  run: {steps: 5.1e7, train_ratio: 32}

procgen:
  task: procgen_coinrun
  run: {steps: 1.1e8, train_ratio: 64}

atari100k:
  task: atari100k_pong
  run: {steps: 1.1e5, envs: 1, train_ratio: 256}

crafter:
  task: crafter_reward
  run: {steps: 1.1e6, envs: 1, train_ratio: 512}

dmc_proprio:
  <<: *size1m
  task: dmc_walker_walk
  env.dmc.image: False
  run: {steps: 1.1e6, train_ratio: 1024}

dmc_vision:
  task: dmc_walker_walk
  env.dmc.proprio: False
  run: {steps: 1.1e6, train_ratio: 256}

bsuite:
  task: bsuite_mnist/0
  run: {envs: 1, save_every: -1, train_ratio: 1024}

loconav:
  task: loconav_ant_maze_m
  env.loconav.repeat: 1
  run.train_ratio: 256

# FPV Navigation
fpv:
  task: fpv_navigation
  batch_size: 16
  batch_length: 64
  replay.size: 1e6
  run:
    envs: 1
    eval_envs: 1
    train_ratio: 128
    steps: 1e6
    log_every: 60
    save_every: 600
    video_every: 4096
    actor_threads: 1
    debug: True
  agent:
    loss_scales: {rec: 1.0, rew: 1.0, con: 1.0, dyn: 1.0, rep: 0.1, policy: 1.0, value: 1.0, repval: 0.3, aux_state: 0.0, aux_goal: 0.0}
    opt: {lr: 4e-5, agc: 0.3, eps: 1e-20, beta1: 0.9, beta2: 0.999, momentum: True, wd: 0.0, schedule: const, warmup: 1000, anneal: 0}
    ac_grads: False
    dyn:
      typ: rssm
      rssm: {deter: 4096, hidden: 512, classes: 32, stoch: 16, act: silu, norm: rms, unimix: 0.01, outscale: 1.0, winit: trunc_normal_in, imglayers: 2, obslayers: 1, dynlayers: 1, absolute: False, blocks: 8, free_nats: 1.0}
    enc:
      typ: simple
      simple: {depth: 64, mults: [2, 3, 4, 4], layers: 3, units: 1024, act: silu, norm: rms, winit: trunc_normal_in, symlog: True, outer: False, kernel: 5, strided: False}
    dec:
      typ: simple
      simple: {depth: 64, mults: [2, 3, 4, 4], layers: 3, units: 1024, act: silu, norm: rms, outscale: 1.0, winit: trunc_normal_in, outer: False, kernel: 5, bspace: 8, strided: False}
    rewhead: {layers: 1, units: 1024, act: silu, norm: rms, output: symexp_twohot, outscale: 0.0, winit: trunc_normal_in, bins: 255}
    conhead: {layers: 1, units: 1024, act: silu, norm: rms, output: binary, outscale: 1.0, winit: trunc_normal_in}
    policy: {layers: 3, units: 1024, act: silu, norm: rms, minstd: 0.1, maxstd: 1.0, outscale: 0.01, unimix: 0.01, winit: trunc_normal_in}
    value: {layers: 3, units: 1024, act: silu, norm: rms, output: symexp_twohot, outscale: 0.0, winit: trunc_normal_in, bins: 255}
    policy_dist_disc: categorical
    policy_dist_cont: bounded_normal
    imag_last: 0
    imag_length: 32
    horizon: 600
    contdisc: True
    imag_loss: {slowtar: False, lam: 0.95, actent: 1e-3, slowreg: 1.0}
    repl_loss: {slowtar: False, lam: 0.95, slowreg: 1.0}
    slowvalue: {rate: 0.02, every: 1}
    retnorm: {impl: perc, rate: 0.01, limit: 1.0, perclo: 5.0, perchi: 95.0, debias: False}
    valnorm: {impl: none, rate: 0.01, limit: 1e-8}
    advnorm: {impl: none, rate: 0.01, limit: 1e-8}
    reward_grad: True
    repval_loss: True
    repval_grad: True
    report: True
    report_gradnorms: False

  jax: {platform: 'cuda', policy_devices: [0], train_devices: [0], prealloc: True, precompile: True}

pretrain_wm:
  script: train_offline
  demo_data_dir: ''
  replay_context: 0
  run: {envs: 0, train_ratio: 512, steps: 500000}
  agent.loss_scales: {rec: 1.0, rew: 1.0, con: 1.0, dyn: 1.0, rep: 0.1, policy: 0.0, value: 0.0, repval: 0.0, aux_state: 1.0, aux_goal: 1.0}

multicpu:
  batch_size: 12
  jax.mock_devices: 8
  jax.policy_devices: [0, 1]
  jax.train_devices: [2, 3, 4, 5, 6, 7]

debug:
  batch_size: 8
  batch_length: 10
  report_length: 5
  jax: {platform: cpu, debug: True, prealloc: False}
  run: {envs: 4, report_every: 10, log_every: 5, save_every: 15, train_ratio: 8, debug: True}
  replay.size: 1e4
  agent:
    .*\.bins: 5
    .*\.layers: 1
    .*\.units: 8
    .*\.stoch: 2
    .*\.classes: 4
    .*\.deter: 8
    .*\.hidden: 3
    .*\.blocks: 4
    .*\.depth: 2